{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6YrTAyzCPA9"
   },
   "source": [
    "# Lab Assignment 10\n",
    "\n",
    "## Objective\n",
    "Your task is to classify images from the CIFAR-10 dataset into 10 different classes (e.g., airplane, automobile, bird, etc.) using a custom implementation of the VGGNet architecture in PyTorch. The goal is to achieve high accuracy on both the validation and test datasets while learning the process of implementing deep learning models from scratch.\n",
    "\n",
    "## Guidelines and Hints:\n",
    "\n",
    "### Framework\n",
    "Use PyTorch to implement the model, as it provides easy-to-use tools for building, training, and testing deep learning models.\n",
    "\n",
    "### Dataset\n",
    "CIFAR-10 consists of 60,000 images (32x32 in size) categorized into 10 classes, with 50,000 images for training and 10,000 images for testing. The dataset is available in torchvision.datasets.CIFAR10.\n",
    "\n",
    "### Approach\n",
    "* Preprocess the dataset (resize images, normalize, apply data augmentation).\n",
    "* Implement the VGGNet architecture (VGG16) using torch.nn.\n",
    "* Use appropriate training techniques such as dropout and data augmentation to reduce overfitting.\n",
    "* Train the model using cross-entropy loss and an optimizer of your choice (e.g., SGD, Adam).\n",
    "* Evaluate the model on the validation dataset after each epoch and test it after training.\n",
    "\n",
    "### Pseudo Code\n",
    "Here’s a step-by-step outline to guide your implementation:\n",
    "\n",
    "* Import Libraries:\n",
    "  * Import necessary PyTorch libraries like torch, torchvision, torch.nn, and torch.optim.\n",
    "\n",
    "* Data Loading and Preprocessing:\n",
    "  * Define transformations using torchvision.transforms for training and testing datasets.\n",
    "  * Load the CIFAR-10 dataset using torchvision.datasets.CIFAR10 and split it into training, validation, and test datasets.\n",
    "  * Use torch.utils.data.DataLoader to create data loaders for each dataset.\n",
    "\n",
    "* Define the VGGNet Model:\n",
    "  * Create a class VGG16_NET that inherits from torch.nn.Module.\n",
    "  * Define convolutional layers, max pooling layers, and fully connected layers following the VGG16 architecture.\n",
    "  * Use nn.ReLU for activation, nn.MaxPool2d for pooling, and nn.Linear for fully connected layers.\n",
    "\n",
    "* Model Training:\n",
    "  * Define a loss function using torch.nn.CrossEntropyLoss.\n",
    "  * Use an optimizer like torch.optim.Adam or torch.optim.SGD.\n",
    "  * Iterate over the training dataset for multiple epochs. For each batch:\n",
    "  * Move the data to the device (CPU/GPU).\n",
    "  * Forward propagate the data through the model.\n",
    "  * Compute the loss and backpropagate to update the weights.\n",
    "  * Log training progress.\n",
    "\n",
    "* Model Evaluation:\n",
    "  * After each epoch, evaluate the model on the validation dataset.\n",
    "  * Use `torch.no_grad()` to avoid computing gradients during evaluation.\n",
    "  * Compute accuracy by comparing predictions with true labels.\n",
    "\n",
    "* Test the Model:\n",
    "  * Evaluate the final trained model on the test dataset and report the test accuracy.\n",
    "\n",
    "## Hints:\n",
    "* Transformations: Use `torchvision.transforms` to resize images to 224x224 and normalize pixel values using mean and standard deviation of ImageNet (as VGG was trained on ImageNet). You can also apply augmentations like `RandomHorizontalFlip`.\n",
    "\n",
    "* Data Loading: Use `torch.utils.data.DataLoader` to create data loaders. Set `shuffle=True` for training data to ensure batches are randomized.\n",
    "\n",
    "* Training Loop: Use the following functions for essential operations:\n",
    "  * `model(images)` for forward propagation.\n",
    "  * `loss.backward()` for backpropagation.\n",
    "  * `optimizer.step()` to update model parameters.\n",
    "\n",
    "* Model Evaluation: Use `outputs.max(1)` to get the class with the highest probability from the model's output.\n",
    "\n",
    "* Device Handling: Check for GPU availability using torch.cuda.is_available() and move data and models to the appropriate device using .to(device)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7KvZn-Wul22",
    "outputId": "ae8232cc-d3e7-4dd7-8b8e-befa6c842874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:03<00:00, 44.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "### WRITE CODE HERE ###\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define data transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.7),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset and split into train and validation sets\n",
    "torch.manual_seed(2024)\n",
    "train = torchvision.datasets.CIFAR10(\"data/\", train=True, download=True, \n",
    "                                     transform=transform_train)\n",
    "\n",
    "val_size = 10000\n",
    "train_size = len(train) - val_size\n",
    "train, val = random_split(train, [train_size, val_size])\n",
    "\n",
    "test = torchvision.datasets.CIFAR10(\"data/\", train=False, download=True, \n",
    "                                    transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpqFgGjH9B6Q"
   },
   "outputs": [],
   "source": [
    "# Define the VGG16-based model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGG16_NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16_NET, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, \n",
    "                                    kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, \n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, \n",
    "                                kernel_size=3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, \n",
    "                                kernel_size=3, padding=1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=512, \n",
    "                                kernel_size=3, padding=1)\n",
    "        self.conv13 = nn.Conv2d(in_channels=512, out_channels=512, \n",
    "                                kernel_size=3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc14 = nn.Linear(25088, 4096)\n",
    "        self.fc15 = nn.Linear(4096, 4096)\n",
    "        self.fc16 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.relu(self.conv13(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc14(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc15(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmhQh6wm9XZy",
    "outputId": "871e15e6-b4e4-4717-d02e-ff02e4627adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] || Step [64/625] || Loss:2.259603075683117\n",
      "Epoch [1/3] || Step [128/625] || Loss:2.1456460868939757\n",
      "Epoch [1/3] || Step [192/625] || Loss:2.056329576919476\n",
      "Epoch [1/3] || Step [256/625] || Loss:1.9933002782054245\n",
      "Epoch [1/3] || Step [320/625] || Loss:1.9439113698899746\n",
      "Epoch [1/3] || Step [384/625] || Loss:1.9061478913451235\n",
      "Epoch [1/3] || Step [448/625] || Loss:1.8702877235731907\n",
      "Epoch [1/3] || Step [512/625] || Loss:1.841114955721423\n",
      "Epoch [1/3] || Step [576/625] || Loss:1.8104227226641443\n",
      "Loss at epoch 1 || 1.790208109664917\n",
      "Validation accuracy 45.58 percentage || Correct 4558 out of 10000 samples\n",
      "Epoch [2/3] || Step [64/625] || Loss:1.4544031880795956\n",
      "Epoch [2/3] || Step [128/625] || Loss:1.458205558359623\n",
      "Epoch [2/3] || Step [192/625] || Loss:1.4394093609104555\n",
      "Epoch [2/3] || Step [256/625] || Loss:1.4279775535687804\n",
      "Epoch [2/3] || Step [320/625] || Loss:1.4046219799667596\n",
      "Epoch [2/3] || Step [384/625] || Loss:1.3835333770451446\n",
      "Epoch [2/3] || Step [448/625] || Loss:1.3686555219548089\n",
      "Epoch [2/3] || Step [512/625] || Loss:1.348092781379819\n",
      "Epoch [2/3] || Step [576/625] || Loss:1.3325752795984347\n",
      "Loss at epoch 2 || 1.3182448407173157\n",
      "Validation accuracy 58.96 percentage || Correct 5896 out of 10000 samples\n",
      "Epoch [3/3] || Step [64/625] || Loss:1.0976725611835718\n",
      "Epoch [3/3] || Step [128/625] || Loss:1.06377385975793\n",
      "Epoch [3/3] || Step [192/625] || Loss:1.049402781451742\n",
      "Epoch [3/3] || Step [256/625] || Loss:1.0381128629669547\n",
      "Epoch [3/3] || Step [320/625] || Loss:1.024192050471902\n",
      "Epoch [3/3] || Step [384/625] || Loss:1.0159851419739425\n",
      "Epoch [3/3] || Step [448/625] || Loss:1.0030659842970115\n",
      "Epoch [3/3] || Step [512/625] || Loss:0.9929894460365176\n",
      "Epoch [3/3] || Step [576/625] || Loss:0.9823403326380584\n",
      "Loss at epoch 3 || 0.9753584115982056\n",
      "Validation accuracy 68.77 percentage || Correct 6877 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and move it to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VGG16_NET()\n",
    "model = model.to(device=device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    loss_var = 0\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        optimizer.zero_grad()\n",
    "        scores = model(images)\n",
    "\n",
    "        loss = criterion(scores, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_var += loss.item()\n",
    "        if idx % 64 == 63:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}] || \n",
    "                  Step [{idx + 1}/{len(train_loader)}] || \n",
    "                     Loss:{loss_var / (idx+1)}')\n",
    "    print(f\"Loss at epoch {epoch + 1} || {loss_var / len(train_loader)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        samples = 0\n",
    "        for idx, (images, labels) in enumerate(val_loader):\n",
    "            images = images.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == labels).sum()\n",
    "            samples += preds.size(0)\n",
    "        print(f\"Validation accuracy {float(correct) / \n",
    "            float(samples) * 100:.2f} percentage || \n",
    "              Correct {correct} out of {samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjHG1vIH9ZrB",
    "outputId": "cecf5a13-a5e1-4de7-8edc-269c686bd572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 68.24 percentage || Correct 6824 out of 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the test dataset\n",
    "correct = 0\n",
    "samples = 0\n",
    "for idx, (images, labels) in enumerate(test_loader):\n",
    "    images = images.to(device=device)\n",
    "    labels = labels.to(device=device)\n",
    "    outputs = model(images)\n",
    "    _, preds = outputs.max(1)\n",
    "    correct += (preds == labels).sum()\n",
    "    samples += preds.size(0)\n",
    "print(f\"Test accuracy {float(correct) / float(samples) * 100:.2f} percentage \n",
    "                || Correct {correct} out of {samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjBm31n49cC3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
